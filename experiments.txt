python train.py --model-size xsmall --position-start-augmentation False --absolute-position-embedding sinusoidal --ckpt result_xsmall_noaug_sinusoidal1.pt > result_xsmall_noaug_sinusoidal1.txt
python train.py --model-size xsmall --position-start-augmentation False --absolute-position-embedding sinusoidal --ckpt result_xsmall_noaug_sinusoidal2.pt > result_xsmall_noaug_sinusoidal2.txt
python train.py --model-size xsmall --position-start-augmentation False --absolute-position-embedding sinusoidal --ckpt result_xsmall_noaug_sinusoidal3.pt > result_xsmall_noaug_sinusoidal3.txt
python train.py --model-size xsmall --position-start-augmentation False --absolute-position-embedding sinusoidal --ckpt result_xsmall_noaug_sinusoidal4.pt > result_xsmall_noaug_sinusoidal4.txt
python train.py --model-size xsmall --position-start-augmentation False --absolute-position-embedding sinusoidal --ckpt result_xsmall_noaug_sinusoidal5.pt > result_xsmall_noaug_sinusoidal5.txt
python train.py --model-size xsmall --position-start-augmentation False --absolute-position-embedding learned --ckpt result_xsmall_noaug_learned1.pt  > result_xsmall_noaug_learned1.txt
python train.py --model-size xsmall --position-start-augmentation False --absolute-position-embedding learned --ckpt result_xsmall_noaug_learned2.pt  > result_xsmall_noaug_learned2.txt
python train.py --model-size xsmall --position-start-augmentation False --absolute-position-embedding learned --ckpt result_xsmall_noaug_learned3.pt  > result_xsmall_noaug_learned3.txt
python train.py --model-size xsmall --position-start-augmentation False --absolute-position-embedding learned --ckpt result_xsmall_noaug_learned4.pt  > result_xsmall_noaug_learned4.txt
python train.py --model-size xsmall --position-start-augmentation False --absolute-position-embedding learned --ckpt result_xsmall_noaug_learned5.pt  > result_xsmall_noaug_learned5.txt
python train.py --model-size xsmall --position-start-augmentation False --absolute-position-embedding none --ckpt result_xsmall_none1.pt  > result_xsmall_none1.txt
python train.py --model-size xsmall --position-start-augmentation False --absolute-position-embedding none --ckpt result_xsmall_none2.pt  > result_xsmall_none2.txt
python train.py --model-size xsmall --position-start-augmentation False --absolute-position-embedding none --ckpt result_xsmall_none3.pt  > result_xsmall_none3.txt
python train.py --model-size xsmall --position-start-augmentation False --absolute-position-embedding none --ckpt result_xsmall_none4.pt  > result_xsmall_none4.txt
python train.py --model-size xsmall --position-start-augmentation False --absolute-position-embedding none --ckpt result_xsmall_none5.pt  > result_xsmall_none5.txt
python train.py --model-size xsmall --position-start-augmentation True --absolute-position-embedding sinusoidal --ckpt result_xsmall_aug_sinusoidal1.pt  > result_xsmall_aug_sinusoidal1.txt
python train.py --model-size xsmall --position-start-augmentation True --absolute-position-embedding sinusoidal --ckpt result_xsmall_aug_sinusoidal2.pt  > result_xsmall_aug_sinusoidal2.txt
python train.py --model-size xsmall --position-start-augmentation True --absolute-position-embedding sinusoidal --ckpt result_xsmall_aug_sinusoidal3.pt  > result_xsmall_aug_sinusoidal3.txt
python train.py --model-size xsmall --position-start-augmentation True --absolute-position-embedding sinusoidal --ckpt result_xsmall_aug_sinusoidal4.pt  > result_xsmall_aug_sinusoidal4.txt
python train.py --model-size xsmall --position-start-augmentation True --absolute-position-embedding sinusoidal --ckpt result_xsmall_aug_sinusoidal5.pt  > result_xsmall_aug_sinusoidal5.txt
python train.py --model-size xsmall --position-start-augmentation True --absolute-position-embedding learned --ckpt result_xsmall_aug_learned1.pt > result_xsmall_aug_learned1.txt
python train.py --model-size xsmall --position-start-augmentation True --absolute-position-embedding learned --ckpt result_xsmall_aug_learned2.pt > result_xsmall_aug_learned2.txt
python train.py --model-size xsmall --position-start-augmentation True --absolute-position-embedding learned --ckpt result_xsmall_aug_learned3.pt > result_xsmall_aug_learned3.txt
python train.py --model-size xsmall --position-start-augmentation True --absolute-position-embedding learned --ckpt result_xsmall_aug_learned4.pt > result_xsmall_aug_learned4.txt
python train.py --model-size xsmall --position-start-augmentation True --absolute-position-embedding learned --ckpt result_xsmall_aug_learned5.pt > result_xsmall_aug_learned5.txt

# after having run the previous experiments, takeaways:
avg avg perplexity for length 128 across runs:
none:
137.06
learned:
137.65
sinusoidal:
138.04

overall:

differences minimal at this model size / training steps / hyperparams

none and aug sinusoidal are potentially the best

but none has an advantage of being much simpler (do nothing)

extra experiments:
xsmall noaug vs aug learned, 50k steps (x5 training steps)

noaug was ~2.9% better than aug after 50k steps (aug len 128 76.065 avg avg ppl vs noaug len 128 73.881 avg avg ppl)

noaug was significantly better at long context (1024 len) (266.99 vs 298.29 avg avg ppl)
but prev experiments showed long context results MUCH more noisy than training context results

next set of tests: relative pos embeddings with none for absolute embeddings
additionally, modifying test lengths, since these methods all generalize much better

python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb --ckpt result_xsmall_none_linear_cpb1.pt > result_xsmall_none_linear_cpb1.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb --ckpt result_xsmall_none_linear_cpb2.pt > result_xsmall_none_linear_cpb2.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb --ckpt result_xsmall_none_linear_cpb3.pt > result_xsmall_none_linear_cpb3.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb --ckpt result_xsmall_none_linear_cpb4.pt > result_xsmall_none_linear_cpb4.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb --ckpt result_xsmall_none_linear_cpb5.pt > result_xsmall_none_linear_cpb5.txt

python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding log_cpb --ckpt result_xsmall_none_log_cpb1.pt > result_xsmall_none_log_cpb1.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding log_cpb --ckpt result_xsmall_none_log_cpb2.pt > result_xsmall_none_log_cpb2.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding log_cpb --ckpt result_xsmall_none_log_cpb3.pt > result_xsmall_none_log_cpb3.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding log_cpb --ckpt result_xsmall_none_log_cpb4.pt > result_xsmall_none_log_cpb4.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding log_cpb --ckpt result_xsmall_none_log_cpb5.pt > result_xsmall_none_log_cpb5.txt

python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding fourier_cpb --ckpt result_xsmall_none_fourier_cpb1.pt > result_xsmall_none_fourier_cpb1.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding fourier_cpb --ckpt result_xsmall_none_fourier_cpb2.pt > result_xsmall_none_fourier_cpb2.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding fourier_cpb --ckpt result_xsmall_none_fourier_cpb3.pt > result_xsmall_none_fourier_cpb3.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding fourier_cpb --ckpt result_xsmall_none_fourier_cpb4.pt > result_xsmall_none_fourier_cpb4.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding fourier_cpb --ckpt result_xsmall_none_fourier_cpb5.pt > result_xsmall_none_fourier_cpb5.txt

python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding alibi --ckpt result_xsmall_none_alibi1.pt > result_xsmall_none_alibi1.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding alibi --ckpt result_xsmall_none_alibi2.pt > result_xsmall_none_alibi2.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding alibi --ckpt result_xsmall_none_alibi3.pt > result_xsmall_none_alibi3.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding alibi --ckpt result_xsmall_none_alibi4.pt > result_xsmall_none_alibi4.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding alibi --ckpt result_xsmall_none_alibi5.pt > result_xsmall_none_alibi5.txt

python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding rotary --ckpt result_xsmall_none_rotary1.pt > result_xsmall_none_rotary1.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding rotary --ckpt result_xsmall_none_rotary2.pt > result_xsmall_none_rotary2.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding rotary --ckpt result_xsmall_none_rotary3.pt > result_xsmall_none_rotary3.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding rotary --ckpt result_xsmall_none_rotary4.pt > result_xsmall_none_rotary4.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding rotary --ckpt result_xsmall_none_rotary5.pt > result_xsmall_none_rotary5.txt

Small transformer LM model trained on seqlen 128 for 10k steps at batch size 128
vocabulary size 8192
n_layers = 6
width = 384
n_heads = 6

avg avg ppl for length 2048 across 5 runs
linear_cpb:     116.56259 +- 0.93250
log_cpb:        117.80258 +- 1.20369
fourier_cpb:    132.80323 +- 2.95787
alibi:          119.69891 +- 0.79353
rotary:         280.03073 +- 6.81101


avg avg ppl for length 128 across 5 runs
linear_cpb:     119.99844 +- 0.73475
log_cpb:        120.72210 +- 1.13642
fourier_cpb:    119.18559 +- 0.98430
alibi:          123.93904 +- 0.66219
rotary:         121.57613 +- 0.81206

extra notes:
linear_cpb advantage over log_cpb actually increases from length 128 to 2048 (~.72 lower ppl to ~1.24 lower ppl)
linear_cpb seems to have lower stdev than the other cpb methods (more consistent)
at training length:
    fourier_cpb is better than log_cpb
    linear_cpb is somewhere between, stdev too large to know
    rotary is beaten by all *_cpb methods!
at length 2048
    fourier_cpb is definitely much worse than both log and linear cpb
    linear_cpb is probably better than log_cpb

Test out linear_cpb but with larger MLP for 15 runs since comparison might be close
Additionally train the other models 10 more times to get total 15 runs per model to get better understanding

python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb_large --ckpt result_xsmall_none_linear_cpb_large1.pt > result_xsmall_none_linear_cpb_large1.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb_large --ckpt result_xsmall_none_linear_cpb_large2.pt > result_xsmall_none_linear_cpb_large2.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb_large --ckpt result_xsmall_none_linear_cpb_large3.pt > result_xsmall_none_linear_cpb_large3.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb_large --ckpt result_xsmall_none_linear_cpb_large4.pt > result_xsmall_none_linear_cpb_large4.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb_large --ckpt result_xsmall_none_linear_cpb_large5.pt > result_xsmall_none_linear_cpb_large5.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb_large --ckpt result_xsmall_none_linear_cpb_large6.pt > result_xsmall_none_linear_cpb_large6.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb_large --ckpt result_xsmall_none_linear_cpb_large7.pt > result_xsmall_none_linear_cpb_large7.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb_large --ckpt result_xsmall_none_linear_cpb_large8.pt > result_xsmall_none_linear_cpb_large8.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb_large --ckpt result_xsmall_none_linear_cpb_large9.pt > result_xsmall_none_linear_cpb_large9.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb_large --ckpt result_xsmall_none_linear_cpb_large10.pt > result_xsmall_none_linear_cpb_large10.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb_large --ckpt result_xsmall_none_linear_cpb_large11.pt > result_xsmall_none_linear_cpb_large11.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb_large --ckpt result_xsmall_none_linear_cpb_large12.pt > result_xsmall_none_linear_cpb_large12.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb_large --ckpt result_xsmall_none_linear_cpb_large13.pt > result_xsmall_none_linear_cpb_large13.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb_large --ckpt result_xsmall_none_linear_cpb_large14.pt > result_xsmall_none_linear_cpb_large14.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb_large --ckpt result_xsmall_none_linear_cpb_large15.pt > result_xsmall_none_linear_cpb_large15.txt


python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb --ckpt result_xsmall_none_linear_cpb6.pt > result_xsmall_none_linear_cpb6.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb --ckpt result_xsmall_none_linear_cpb7.pt > result_xsmall_none_linear_cpb7.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb --ckpt result_xsmall_none_linear_cpb8.pt > result_xsmall_none_linear_cpb8.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb --ckpt result_xsmall_none_linear_cpb9.pt > result_xsmall_none_linear_cpb9.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb --ckpt result_xsmall_none_linear_cpb10.pt > result_xsmall_none_linear_cpb10.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb --ckpt result_xsmall_none_linear_cpb11.pt > result_xsmall_none_linear_cpb11.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb --ckpt result_xsmall_none_linear_cpb12.pt > result_xsmall_none_linear_cpb12.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb --ckpt result_xsmall_none_linear_cpb13.pt > result_xsmall_none_linear_cpb13.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb --ckpt result_xsmall_none_linear_cpb14.pt > result_xsmall_none_linear_cpb14.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding linear_cpb --ckpt result_xsmall_none_linear_cpb15.pt > result_xsmall_none_linear_cpb15.txt

python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding log_cpb --ckpt result_xsmall_none_log_cpb6.pt > result_xsmall_none_log_cpb6.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding log_cpb --ckpt result_xsmall_none_log_cpb7.pt > result_xsmall_none_log_cpb7.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding log_cpb --ckpt result_xsmall_none_log_cpb8.pt > result_xsmall_none_log_cpb8.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding log_cpb --ckpt result_xsmall_none_log_cpb9.pt > result_xsmall_none_log_cpb9.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding log_cpb --ckpt result_xsmall_none_log_cpb10.pt > result_xsmall_none_log_cpb10.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding log_cpb --ckpt result_xsmall_none_log_cpb11.pt > result_xsmall_none_log_cpb11.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding log_cpb --ckpt result_xsmall_none_log_cpb12.pt > result_xsmall_none_log_cpb12.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding log_cpb --ckpt result_xsmall_none_log_cpb13.pt > result_xsmall_none_log_cpb13.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding log_cpb --ckpt result_xsmall_none_log_cpb14.pt > result_xsmall_none_log_cpb14.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding log_cpb --ckpt result_xsmall_none_log_cpb15.pt > result_xsmall_none_log_cpb15.txt

python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding fourier_cpb --ckpt result_xsmall_none_fourier_cpb6.pt > result_xsmall_none_fourier_cpb6.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding fourier_cpb --ckpt result_xsmall_none_fourier_cpb7.pt > result_xsmall_none_fourier_cpb7.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding fourier_cpb --ckpt result_xsmall_none_fourier_cpb8.pt > result_xsmall_none_fourier_cpb8.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding fourier_cpb --ckpt result_xsmall_none_fourier_cpb9.pt > result_xsmall_none_fourier_cpb9.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding fourier_cpb --ckpt result_xsmall_none_fourier_cpb10.pt > result_xsmall_none_fourier_cpb10.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding fourier_cpb --ckpt result_xsmall_none_fourier_cpb11.pt > result_xsmall_none_fourier_cpb11.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding fourier_cpb --ckpt result_xsmall_none_fourier_cpb12.pt > result_xsmall_none_fourier_cpb12.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding fourier_cpb --ckpt result_xsmall_none_fourier_cpb13.pt > result_xsmall_none_fourier_cpb13.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding fourier_cpb --ckpt result_xsmall_none_fourier_cpb14.pt > result_xsmall_none_fourier_cpb14.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding fourier_cpb --ckpt result_xsmall_none_fourier_cpb15.pt > result_xsmall_none_fourier_cpb15.txt

python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding alibi --ckpt result_xsmall_none_alibi6.pt > result_xsmall_none_alibi6.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding alibi --ckpt result_xsmall_none_alibi7.pt > result_xsmall_none_alibi7.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding alibi --ckpt result_xsmall_none_alibi8.pt > result_xsmall_none_alibi8.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding alibi --ckpt result_xsmall_none_alibi9.pt > result_xsmall_none_alibi9.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding alibi --ckpt result_xsmall_none_alibi10.pt > result_xsmall_none_alibi10.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding alibi --ckpt result_xsmall_none_alibi11.pt > result_xsmall_none_alibi11.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding alibi --ckpt result_xsmall_none_alibi12.pt > result_xsmall_none_alibi12.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding alibi --ckpt result_xsmall_none_alibi13.pt > result_xsmall_none_alibi13.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding alibi --ckpt result_xsmall_none_alibi14.pt > result_xsmall_none_alibi14.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding alibi --ckpt result_xsmall_none_alibi15.pt > result_xsmall_none_alibi15.txt

python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding rotary --ckpt result_xsmall_none_rotary6.pt > result_xsmall_none_rotary6.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding rotary --ckpt result_xsmall_none_rotary7.pt > result_xsmall_none_rotary7.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding rotary --ckpt result_xsmall_none_rotary8.pt > result_xsmall_none_rotary8.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding rotary --ckpt result_xsmall_none_rotary9.pt > result_xsmall_none_rotary9.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding rotary --ckpt result_xsmall_none_rotary10.pt > result_xsmall_none_rotary10.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding rotary --ckpt result_xsmall_none_rotary11.pt > result_xsmall_none_rotary11.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding rotary --ckpt result_xsmall_none_rotary12.pt > result_xsmall_none_rotary12.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding rotary --ckpt result_xsmall_none_rotary13.pt > result_xsmall_none_rotary13.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding rotary --ckpt result_xsmall_none_rotary14.pt > result_xsmall_none_rotary14.txt
python train.py --model-size xsmall --absolute-position-embedding none --relative-position-embedding rotary --ckpt result_xsmall_none_rotary15.pt > result_xsmall_none_rotary15.txt

avg avg ppl for length 2048 across 15 runs
linear_cpb:     116.00805 +- 0.86725
linear_cpb_l:   116.83956 +- 0.88274
log_cpb:        117.00886 +- 1.23997
fourier_cpb:    133.25709 +- 2.71880
alibi:          120.05529 +- 0.93290
rotary:         279.71282 +- 8.55629


avg avg ppl for length 128 across 15 runs
linear_cpb:     119.57854 +- 0.73356
linear_cpb_l:   120.30822 +- 0.79160
log_cpb:        120.05336 +- 1.07849
fourier_cpb:    119.17091 +- 0.85088
alibi:          124.17228 +- 0.81266
rotary:         121.61846 +- 0.91422

Test at larger scales, remove a few options:

No more fourier_cpb, while (very slightly) better at training length, we care more about extrapolation here
No more linear_cpb_l, the extra size obviously does not help
No more log_cpb, linear has consistently slightly outpaced it, and they are very similar either way

Testing linear_cpb, the best new contender ( it may worsen with scale)
Testing alibi, the previous best extrapolator (it may improve with scale)
Testing rotary, the previous best for training length (it may improve with scale)

To sum it up, I have removed all the variants of cpb except for the best variant, linear_cpb
It will be compared against the previous best embeddings, alibi for extrapolation and rotary for training length
By reducing the number of variants it greatly reduces # of models trained, which is important as model scale increases
As I have limited computing power

Nevermind, redoing previous experiements due to model changes.

Just 5 experiments this time, but with fixed seeds to aid comparisons across models

python train.py --model-size xsmall --rel-pos-embed linear_cpb --ckpt result_xsmall_linear_cpb1.pt --seed 42 > result_xsmall_linear_cpb1.txt
python train.py --model-size xsmall --rel-pos-embed linear_cpb --ckpt result_xsmall_linear_cpb2.pt --seed 1337 > result_xsmall_linear_cpb2.txt
python train.py --model-size xsmall --rel-pos-embed linear_cpb --ckpt result_xsmall_linear_cpb3.pt --seed 3407 > result_xsmall_linear_cpb3.txt
python train.py --model-size xsmall --rel-pos-embed linear_cpb --ckpt result_xsmall_linear_cpb4.pt --seed 1024 > result_xsmall_linear_cpb4.txt
python train.py --model-size xsmall --rel-pos-embed linear_cpb --ckpt result_xsmall_linear_cpb5.pt --seed 365 > result_xsmall_linear_cpb5.txt

python train.py --model-size xsmall --rel-pos-embed log_cpb --ckpt result_xsmall_log_cpb1.pt --seed 42 > result_xsmall_log_cpb1.txt
python train.py --model-size xsmall --rel-pos-embed log_cpb --ckpt result_xsmall_log_cpb2.pt --seed 1337 > result_xsmall_log_cpb2.txt
python train.py --model-size xsmall --rel-pos-embed log_cpb --ckpt result_xsmall_log_cpb3.pt --seed 3407 > result_xsmall_log_cpb3.txt
python train.py --model-size xsmall --rel-pos-embed log_cpb --ckpt result_xsmall_log_cpb4.pt --seed 1024 > result_xsmall_log_cpb4.txt
python train.py --model-size xsmall --rel-pos-embed log_cpb --ckpt result_xsmall_log_cpb5.pt --seed 365 > result_xsmall_log_cpb5.txt

python train.py --model-size xsmall --rel-pos-embed fourier_cpb --ckpt result_xsmall_fourier_cpb1.pt --seed 42 > result_xsmall_fourier_cpb1.txt
python train.py --model-size xsmall --rel-pos-embed fourier_cpb --ckpt result_xsmall_fourier_cpb2.pt --seed 1337 > result_xsmall_fourier_cpb2.txt
python train.py --model-size xsmall --rel-pos-embed fourier_cpb --ckpt result_xsmall_fourier_cpb3.pt --seed 3407 > result_xsmall_fourier_cpb3.txt
python train.py --model-size xsmall --rel-pos-embed fourier_cpb --ckpt result_xsmall_fourier_cpb4.pt --seed 1024 > result_xsmall_fourier_cpb4.txt
python train.py --model-size xsmall --rel-pos-embed fourier_cpb --ckpt result_xsmall_fourier_cpb5.pt --seed 365 > result_xsmall_fourier_cpb5.txt

python train.py --model-size xsmall --rel-pos-embed alibi --ckpt result_xsmall_alibi1.pt --seed 42 > result_xsmall_alibi1.txt
python train.py --model-size xsmall --rel-pos-embed alibi --ckpt result_xsmall_alibi2.pt --seed 1337 > result_xsmall_alibi2.txt
python train.py --model-size xsmall --rel-pos-embed alibi --ckpt result_xsmall_alibi3.pt --seed 3407 > result_xsmall_alibi3.txt
python train.py --model-size xsmall --rel-pos-embed alibi --ckpt result_xsmall_alibi4.pt --seed 1024 > result_xsmall_alibi4.txt
python train.py --model-size xsmall --rel-pos-embed alibi --ckpt result_xsmall_alibi5.pt --seed 365 > result_xsmall_alibi5.txt

python train.py --model-size xsmall --rel-pos-embed rotary --ckpt result_xsmall_rotary1.pt --seed 42 > result_xsmall_rotary1.txt
python train.py --model-size xsmall --rel-pos-embed rotary --ckpt result_xsmall_rotary2.pt --seed 1337 > result_xsmall_rotary2.txt
python train.py --model-size xsmall --rel-pos-embed rotary --ckpt result_xsmall_rotary3.pt --seed 3407 > result_xsmall_rotary3.txt
python train.py --model-size xsmall --rel-pos-embed rotary --ckpt result_xsmall_rotary4.pt --seed 1024 > result_xsmall_rotary4.txt
python train.py --model-size xsmall --rel-pos-embed rotary --ckpt result_xsmall_rotary5.pt --seed 365 > result_xsmall_rotary5.txt

And up a scale

python train.py --model-size small --rel-pos-embed linear_cpb --ckpt result_small_linear_cpb1.pt --seed 42 > result_small_linear_cpb1.txt
python train.py --model-size small --rel-pos-embed linear_cpb --ckpt result_small_linear_cpb2.pt --seed 1337 > result_small_linear_cpb2.txt
python train.py --model-size small --rel-pos-embed linear_cpb --ckpt result_small_linear_cpb3.pt --seed 3407 > result_small_linear_cpb3.txt
python train.py --model-size small --rel-pos-embed linear_cpb --ckpt result_small_linear_cpb4.pt --seed 1024 > result_small_linear_cpb4.txt
python train.py --model-size small --rel-pos-embed linear_cpb --ckpt result_small_linear_cpb5.pt --seed 365 > result_small_linear_cpb5.txt

python train.py --model-size small --rel-pos-embed log_cpb --ckpt result_small_log_cpb1.pt --seed 42 > result_small_log_cpb1.txt
python train.py --model-size small --rel-pos-embed log_cpb --ckpt result_small_log_cpb2.pt --seed 1337 > result_small_log_cpb2.txt
python train.py --model-size small --rel-pos-embed log_cpb --ckpt result_small_log_cpb3.pt --seed 3407 > result_small_log_cpb3.txt
python train.py --model-size small --rel-pos-embed log_cpb --ckpt result_small_log_cpb4.pt --seed 1024 > result_small_log_cpb4.txt
python train.py --model-size small --rel-pos-embed log_cpb --ckpt result_small_log_cpb5.pt --seed 365 > result_small_log_cpb5.txt

python train.py --model-size small --rel-pos-embed fourier_cpb --ckpt result_small_fourier_cpb1.pt --seed 42 > result_small_fourier_cpb1.txt
python train.py --model-size small --rel-pos-embed fourier_cpb --ckpt result_small_fourier_cpb2.pt --seed 1337 > result_small_fourier_cpb2.txt
python train.py --model-size small --rel-pos-embed fourier_cpb --ckpt result_small_fourier_cpb3.pt --seed 3407 > result_small_fourier_cpb3.txt
python train.py --model-size small --rel-pos-embed fourier_cpb --ckpt result_small_fourier_cpb4.pt --seed 1024 > result_small_fourier_cpb4.txt
python train.py --model-size small --rel-pos-embed fourier_cpb --ckpt result_small_fourier_cpb5.pt --seed 365 > result_small_fourier_cpb5.txt

python train.py --model-size small --rel-pos-embed alibi --ckpt result_small_alibi1.pt --seed 42 > result_small_alibi1.txt
python train.py --model-size small --rel-pos-embed alibi --ckpt result_small_alibi2.pt --seed 1337 > result_small_alibi2.txt
python train.py --model-size small --rel-pos-embed alibi --ckpt result_small_alibi3.pt --seed 3407 > result_small_alibi3.txt
python train.py --model-size small --rel-pos-embed alibi --ckpt result_small_alibi4.pt --seed 1024 > result_small_alibi4.txt
python train.py --model-size small --rel-pos-embed alibi --ckpt result_small_alibi5.pt --`seed 365 > result_small_alibi5.txt

python train.py --model-size small --rel-pos-embed rotary --ckpt result_small_rotary1.pt --seed 42 > result_small_rotary1.txt
python train.py --model-size small --rel-pos-embed rotary --ckpt result_small_rotary2.pt --seed 1337 > result_small_rotary2.txt
python train.py --model-size small --rel-pos-embed rotary --ckpt result_small_rotary3.pt --seed 3407 > result_small_rotary3.txt
python train.py --model-size small --rel-pos-embed rotary --ckpt result_small_rotary4.pt --seed 1024 > result_small_rotary4.txt
python train.py --model-size small --rel-pos-embed rotary --ckpt result_small_rotary5.pt --seed 365 > result_small_rotary5.txt

Trained with long context length
Context length of 2048.

To maintain same # tokens per batch, reduce batch size down to 8, same # of tokens as 128 ctx with 128 batch

python train.py --model-size xsmall --train-ctx-len 2048 --batch-size 8 --rel-pos-embed linear_cpb --ckpt result_xsmall_linear_cpb1.pt --seed 42 > result_xsmall_linear_cpb1.txt

python train.py --model-size xsmall --train-ctx-len 2048 --batch-size 8 --rel-pos-embed log_cpb --ckpt result_xsmall_log_cpb1.pt --seed 42 > result_xsmall_log_cpb1.txt

python train.py --model-size xsmall --train-ctx-len 2048 --batch-size 8 --rel-pos-embed fourier_cpb --ckpt result_xsmall_fourier_cpb1.pt --seed 42 > result_xsmall_fourier_cpb1.txt

python train.py --model-size xsmall --train-ctx-len 2048 --batch-size 8 --rel-pos-embed alibi --ckpt result_xsmall_alibi1.pt --seed 42 > result_xsmall_alibi1.txt

python train.py --model-size xsmall --train-ctx-len 2048 --batch-size 8 --rel-pos-embed rotary --ckpt result_xsmall_rotary1.pt --seed 42 > result_xsmall_rotary1.txt

Result is that longer context got lower loss in same steps and tokens per batch

However, it used significantly more computing power and memory, barely fit into the GPU (~3x memory) and took ~6x longer

Since I saw no improvement with increased context length from models trained on 128, it seems like a good idea
to train some models across various context lengths (128, 256, 512)
Given anthropic's new paper on "Induction Heads" it seems worth it to try and train some models to achieve that
According to their data, it takes ~2.5 to 5 BILLION tokens seen to activate the meta-learning
These models will all be trained for ~6.5 BILLION tokens to ensure that the meta-learning is hit during training

Training some xsmall linear_cpb models:

python train.py --model-size xsmall --train-ctx-len 128 --batch-size 256 --num-train-steps 200000 --rel-pos-embed linear_cpb --ckpt xsmall_linear_cpb_128_1.pt --seed 42 > xsmall_linear_cpb_128_1.txt
python train.py --model-size xsmall --train-ctx-len 256 --batch-size 128 --num-train-steps 200000 --rel-pos-embed linear_cpb --ckpt xsmall_linear_cpb_256_1.pt --seed 42 > xsmall_linear_cpb_256_1.txt
python train.py --model-size xsmall --train-ctx-len 512 --batch-size 64 --num-train-steps 200000 --rel-pos-embed linear_cpb --ckpt xsmall_linear_cpb_512_1.pt --seed 42 > xsmall_linear_cpb_512_1.txt
python train.py --model-size xsmall --train-ctx-len 1024 --batch-size 32 --num-train-steps 200000 --rel-pos-embed linear_cpb --ckpt xsmall_linear_cpb_1024_1.pt --seed 42 > xsmall_linear_cpb_1024_1.txt

Results:
Length 128 had minimal meta-learning, and there was much noise.
Additionally, the slope of loss was positive (after train length), so loss would increase with length

Length 256 had much more metalearning, and lower noise, but not nearly as big a loss jump as Anthropic saw.
This may be from multiple factors which I will list:
1. Smaller training set, multiple epochs had to be done
2. Smaller vocabulary / lower loss.
Anthropic likely used a larger vocabulary (or harder dataset) as their loss was much higher.
This means that the meta-learning loss should be smaller, relative to the training/testing loss'

The slope was around -5.2e-6

Length 512 had a decent jump in meta learning loss @ 500:
-0.125 (256) to -0.163, around a 30% improvement.  Interestingly, it also took ~30% longer to train

The slope did not improve much, only to around -5.9e-6

Length 1024 was interesting.  The improvement in meta learning loss was not much, only to ~-.18

Interestingly, the slope improved considerably, to around -8.2e-6.  Consdering that 256->512 was only ~.7 different,
I would have expected somewhere closer to ~-7.3e-6 given a linear trend from 256 to 512, so the change is surprising.
Additionally, given the large improvement between 128 and 256 (+3 -> -5), I expected a decaying improvement, so this
jump was unexepected given the previous data.

However, 1024 is starting to approach the test length of 2048 (which was chosen for compute/memory reasons),
so the results may be less surprising with a test length of 4096 or even higher

Especially as the meta-learning relative loss is not so much better than 512

Finetuning:

Fine-tuning models pre-trained for a while on short lengths to longer lengths for a small # of steps
If training a model at length 256 for 99% of training time, and then finetune to 1024 for a few hundred steps works,
then it becomes feasible to train powerful models on modest hardware, by reducing computation requirements 2x or more

256 is 2x faster to train than 1024 with xsmall model on rtx 3090, but with larger models the gap should grow towards
the theoretical limit of 16x (4x longer n, attention is n^2, so 16x longer training time)

python finetune.py --model-size xsmall --train-ctx-len 512 --batch-size 64 --num-train-steps 25 --rel-pos-embed linear_cpb --load-ckpt xsmall_linear_cpb_128_1.pt --save-ckpt xsmall_linear_cpb_128_1_finetuned_512_25steps.pt  --seed 42 > xsmall_linear_cpb_128_1_finetuned_512_25steps.txt
python finetune.py --model-size xsmall --train-ctx-len 512 --batch-size 64 --num-train-steps 100 --rel-pos-embed linear_cpb --load-ckpt xsmall_linear_cpb_128_1.pt --save-ckpt xsmall_linear_cpb_128_1_finetuned_512_100steps.pt  --seed 42 > xsmall_linear_cpb_128_1_finetuned_512_100steps.txt
python finetune.py --model-size xsmall --train-ctx-len 512 --batch-size 64 --num-train-steps 200 --rel-pos-embed linear_cpb --load-ckpt xsmall_linear_cpb_128_1.pt --save-ckpt xsmall_linear_cpb_128_1_finetuned_512_200steps.pt  --seed 42 > xsmall_linear_cpb_128_1_finetuned_512_200steps.txt
python finetune.py --model-size xsmall --train-ctx-len 512 --batch-size 64 --num-train-steps 500 --rel-pos-embed linear_cpb --load-ckpt xsmall_linear_cpb_128_1.pt --save-ckpt xsmall_linear_cpb_128_1_finetuned_512_500steps.pt  --seed 42 > xsmall_linear_cpb_128_1_finetuned_512_500steps.txt
python finetune.py --model-size xsmall --train-ctx-len 512 --batch-size 64 --num-train-steps 1000 --rel-pos-embed linear_cpb --load-ckpt xsmall_linear_cpb_128_1.pt --save-ckpt xsmall_linear_cpb_128_1_finetuned_512_1000steps.pt  --seed 42 > xsmall_linear_cpb_128_1_finetuned_512_1000steps.txt

python finetune.py --model-size xsmall --train-ctx-len 1024 --batch-size 32 --num-train-steps 25 --rel-pos-embed linear_cpb --load-ckpt xsmall_linear_cpb_128_1.pt --save-ckpt xsmall_linear_cpb_128_1_finetuned_1024_25steps.pt  --seed 42 > xsmall_linear_cpb_128_1_finetuned_1024_25steps.txt
python finetune.py --model-size xsmall --train-ctx-len 1024 --batch-size 32 --num-train-steps 100 --rel-pos-embed linear_cpb --load-ckpt xsmall_linear_cpb_128_1.pt --save-ckpt xsmall_linear_cpb_128_1_finetuned_1024_100steps.pt  --seed 42 > xsmall_linear_cpb_128_1_finetuned_1024_100steps.txt
python finetune.py --model-size xsmall --train-ctx-len 1024 --batch-size 32 --num-train-steps 200 --rel-pos-embed linear_cpb --load-ckpt xsmall_linear_cpb_128_1.pt --save-ckpt xsmall_linear_cpb_128_1_finetuned_1024_200steps.pt  --seed 42 > xsmall_linear_cpb_128_1_finetuned_1024_200steps.txt
python finetune.py --model-size xsmall --train-ctx-len 1024 --batch-size 32 --num-train-steps 500 --rel-pos-embed linear_cpb --load-ckpt xsmall_linear_cpb_128_1.pt --save-ckpt xsmall_linear_cpb_128_1_finetuned_1024_500steps.pt  --seed 42 > xsmall_linear_cpb_128_1_finetuned_1024_500steps.txt
python finetune.py --model-size xsmall --train-ctx-len 1024 --batch-size 32 --num-train-steps 1000 --rel-pos-embed linear_cpb --load-ckpt xsmall_linear_cpb_128_1.pt --save-ckpt xsmall_linear_cpb_128_1_finetuned_1024_1000steps.pt  --seed 42 > xsmall_linear_cpb_128_1_finetuned_1024_1000steps.txt


python finetune.py --model-size xsmall --train-ctx-len 512 --batch-size 64 --num-train-steps 25 --rel-pos-embed linear_cpb --load-ckpt xsmall_linear_cpb_256_1.pt --save-ckpt xsmall_linear_cpb_256_1_finetuned_512_25steps.pt  --seed 42 > xsmall_linear_cpb_256_1_finetuned_512_25steps.txt
